{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPCsY/xug53w0royhwBKnXE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install -qU transformers[sentencepiece] sentence-transformers faiss-cpu langchain gradio python-multipart pdfminer.six\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xlz8aTbIyNHY","executionInfo":{"status":"ok","timestamp":1763315371258,"user_tz":-330,"elapsed":16909,"user":{"displayName":"Arjun Ganesh","userId":"09569903898506552821"}},"outputId":"d57786b6-fd49-4986-f0a1-d0bdc2e2386a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.7/93.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.5/471.5 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.3/208.3 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["HF_TOKEN = \"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-hftoken\"\n"],"metadata":{"id":"SbfTg9lHz-AC","executionInfo":{"status":"ok","timestamp":1763315830311,"user_tz":-330,"elapsed":26,"user":{"displayName":"Arjun Ganesh","userId":"09569903898506552821"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":634},"id":"h0J8FB-MxyG-","executionInfo":{"status":"ok","timestamp":1763316495227,"user_tz":-330,"elapsed":3743,"user":{"displayName":"Arjun Ganesh","userId":"09569903898506552821"}},"outputId":"b1ead276-9e68-4a73-b1f7-7c12922141c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["This is a module. To run the Gradio demo, call launch_gradio(hf_token='<YOUR_HF_TOKEN>')\n"]},{"output_type":"stream","name":"stderr","text":["Device set to use cpu\n"]},{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n","* To create a public link, set `share=True` in `launch()`.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["(async (port, path, width, height, cache, element) => {\n","                        if (!google.colab.kernel.accessAllowed && !cache) {\n","                            return;\n","                        }\n","                        element.appendChild(document.createTextNode(''));\n","                        const url = await google.colab.kernel.proxyPort(port, {cache});\n","\n","                        const external_link = document.createElement('div');\n","                        external_link.innerHTML = `\n","                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n","                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n","                                    https://localhost:${port}${path}\n","                                </a>\n","                            </div>\n","                        `;\n","                        element.appendChild(external_link);\n","\n","                        const iframe = document.createElement('iframe');\n","                        iframe.src = new URL(path, url).toString();\n","                        iframe.height = height;\n","                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n","                        iframe.width = width;\n","                        iframe.style.border = 0;\n","                        element.appendChild(iframe);\n","                    })(7861, \"/\", \"100%\", 500, false, window.element)"]},"metadata":{}}],"source":["# GenAI-based Study Assistant (Colab-ready)\n","# Author: ChatGPT\n","# Run this entire file in Google Colab (create a new notebook, paste into a cell or upload as a .py and run).\n","# Features:\n","# - Upload / load documents (txt, pdf via plain text extraction)\n","# - Build FAISS vector store using sentence-transformers embeddings\n","# - Simple Retrieval-Augmented Generation (RAG) using a Hugging Face generation model\n","# - Utilities: summarize document, generate quizzes, ask questions (Q&A)\n","# - Lightweight Gradio UI to interact with the assistant in Colab\n","\n","# --- Instructions before running ---\n","# 1) (Recommended) In Colab, set your Hugging Face token in the following cell or as an environment variable:\n","#    HF_TOKEN = \"<your_huggingface_token>\"\n","#    You can create one at https://huggingface.co/settings/tokens (\"Write\" scope not needed for inference).\n","# 2) Run the notebook. If you get out-of-memory issues, switch to a smaller model (see MODEL_NAME below).\n","\n","# --- Important notes ---\n","# - This script uses CPU / GPU depending on Colab runtime. If you have GPU enabled, transformers will use it.\n","# - Models like 'google/flan-t5-large' are large; consider using 'google/flan-t5-small' or 'tiiuae/falcon-mini' for smaller footprint.\n","\n","# ----------------- Setup & Installs -----------------\n","\n","# If running inside Colab, uncomment and run the installs below. If running locally, install equivalent packages.\n","\n","# !pip install -qU transformers[sentencepiece] sentence-transformers faiss-cpu langchain gradio python-multipart pdfminer.six\n","\n","# ----------------- Imports -----------------\n","from typing import List, Tuple\n","import os\n","import tempfile\n","import math\n","import json\n","\n","# Text processing\n","from pathlib import Path\n","\n","# Transformers for generation\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n","\n","# Sentence Transformers for embeddings\n","from sentence_transformers import SentenceTransformer\n","\n","# Vector store\n","import faiss\n","import numpy as np\n","\n","# PDF reading\n","from pdfminer.high_level import extract_text\n","\n","# Simple web UI\n","import gradio as gr\n","\n","# ----------------- Configuration -----------------\n","# Choose models here. If you have GPU, prefer larger models; otherwise select small models.\n","EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # small and fast\n","# Generation models (seq2seq) - flan variants are good. Choose smaller if memory is limited.\n","GEN_MODEL = \"google/flan-t5-small\"  # change to 'google/flan-t5-large' if you have GPU and memory\n","\n","# Number of tokens to generate for answers\n","MAX_NEW_TOKENS = 256\n","\n","# Number of retrieved docs\n","TOP_K = 4\n","\n","# ----------------- Helpers -----------------\n","\n","def read_text_file(path: str) -> str:\n","    return Path(path).read_text(encoding=\"utf-8\")\n","\n","\n","def read_pdf_file(path: str) -> str:\n","    # extract_text from pdfminer\n","    return extract_text(path)\n","\n","\n","def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n","    \"\"\"Split long text into overlapping chunks (approx chunk_size words).\"\"\"\n","    words = text.split()\n","    chunks = []\n","    i = 0\n","    while i < len(words):\n","        chunk = \" \".join(words[i : i + chunk_size])\n","        chunks.append(chunk)\n","        i += chunk_size - overlap\n","    return chunks\n","\n","# ----------------- Embeddings & FAISS -----------------\n","class FaissIndex:\n","    def __init__(self, embedding_model_name: str = EMBEDDING_MODEL):\n","        self.embedding_model_name = embedding_model_name\n","        self.embedder = SentenceTransformer(embedding_model_name)\n","        self.index = None\n","        self.id2meta = {}\n","        self.dim = self.embedder.get_sentence_embedding_dimension()\n","\n","    def build(self, texts: List[str], metadatas: List[dict] = None):\n","        \"\"\"Build a FAISS index from texts. metadatas is parallel list of dicts.\"\"\"\n","        embs = self.embedder.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n","        self.index = faiss.IndexFlatIP(self.dim)\n","        # normalize for cosine similarity\n","        faiss.normalize_L2(embs)\n","        self.index.add(embs)\n","        self.id2meta = {i: (texts[i], metadatas[i] if metadatas else {}) for i in range(len(texts))}\n","\n","    def add(self, texts: List[str], metadatas: List[dict] = None):\n","        start = len(self.id2meta)\n","        embs = self.embedder.encode(texts, show_progress_bar=False, convert_to_numpy=True)\n","        faiss.normalize_L2(embs)\n","        if self.index is None:\n","            self.index = faiss.IndexFlatIP(self.dim)\n","        self.index.add(embs)\n","        for i, t in enumerate(texts):\n","            self.id2meta[start + i] = (t, (metadatas[i] if metadatas else {}))\n","\n","    def search(self, query: str, k: int = TOP_K) -> List[Tuple[str, float, dict]]:\n","        q_emb = self.embedder.encode([query], convert_to_numpy=True)\n","        faiss.normalize_L2(q_emb)\n","        D, I = self.index.search(q_emb, k)\n","        results = []\n","        for score, idx in zip(D[0], I[0]):\n","            if idx == -1:\n","                continue\n","            text, meta = self.id2meta[idx]\n","            results.append((text, float(score), meta))\n","        return results\n","\n","# ----------------- Generator (RAG) -----------------\n","class RAGGenerator:\n","    def __init__(self, gen_model_name: str = GEN_MODEL, hf_token: str = None):\n","        # If using HF hub model requiring token, set environment variable HUGGINGFACEHUB_API_TOKEN\n","        if hf_token:\n","            os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_token\n","        self.tokenizer = AutoTokenizer.from_pretrained(gen_model_name)\n","        self.model = AutoModelForSeq2SeqLM.from_pretrained(gen_model_name)\n","        self.pipe = pipeline(\n","            \"text2text-generation\",\n","            model=self.model,\n","            tokenizer=self.tokenizer,\n","            device=-1,  # CPU by default; set to 0 for GPU if available and supported\n","        )\n","\n","    def generate_answer(self, question: str, contexts: List[str], max_new_tokens: int = MAX_NEW_TOKENS) -> str:\n","        # Build prompt by concatenating retrieved contexts\n","        prompt = \"\"\"You are a helpful study assistant. Use the following context pieces from documents to answer the question concisely and accurately. If the answer is not present in the context, say 'I don't know based on the provided materials.'\\n\\n\"\"\"\n","        for i, c in enumerate(contexts):\n","            prompt += f\"Context {i+1}: {c}\\n\\n\"\n","        prompt += f\"Question: {question}\\nAnswer:\"\n","        out = self.pipe(prompt, max_new_tokens=max_new_tokens, do_sample=False)\n","        return out[0][\"generated_text\"].strip()\n","\n","    def summarize(self, text: str, max_new_tokens: int = 128) -> str:\n","        prompt = f\"Summarize the following text in clear bullet points:\\n\\n{text}\\n\\nSummary:\"\n","        out = self.pipe(prompt, max_new_tokens=max_new_tokens, do_sample=False)\n","        return out[0][\"generated_text\"].strip()\n","\n","    def generate_quiz(self, text: str, num_questions: int = 5, max_new_tokens: int = 256) -> str:\n","        prompt = (\n","            f\"Create {num_questions} multiple-choice questions (4 choices each) from the following text. Include the correct answer letter.\\n\\n{text}\\n\\nQuestions:\"\n","        )\n","        out = self.pipe(prompt, max_new_tokens=max_new_tokens, do_sample=False)\n","        return out[0][\"generated_text\"].strip()\n","\n","# ----------------- App Logic -----------------\n","\n","# We'll keep a global index and generator instance for the Gradio app session\n","INDEX = None\n","GEN = None\n","DOCUMENTS = []\n","\n","\n","def init(hf_token: str = None):\n","    global INDEX, GEN\n","    INDEX = FaissIndex()\n","    GEN = RAGGenerator(gen_model_name=GEN_MODEL, hf_token=hf_token)\n","    return \"Initialized models (embedder + generator).\"\n","\n","\n","def add_documents_from_text(text: str, source_name: str = \"user_doc\"):\n","    global INDEX, DOCUMENTS\n","    chunks = chunk_text(text, chunk_size=250, overlap=50)\n","    metas = [{\"source\": source_name, \"chunk_id\": i} for i in range(len(chunks))]\n","    if INDEX.index is None:\n","        INDEX.build(chunks, metas)\n","    else:\n","        INDEX.add(chunks, metas)\n","    DOCUMENTS.extend([(source_name, c) for c in chunks])\n","    return f\"Added {len(chunks)} chunks from {source_name}.\"\n","\n","\n","def upload_and_index(file):\n","    # Gradio file input gives a tempfile-like object with .name\n","    p = file.name\n","    suffix = p.split('.')[-1].lower()\n","    if suffix in ['txt', 'md']:\n","        text = read_text_file(p)\n","    elif suffix in ['pdf']:\n","        text = read_pdf_file(p)\n","    else:\n","        # attempt to read as text\n","        try:\n","            text = read_text_file(p)\n","        except Exception as e:\n","            return f\"Unsupported file type: {e}\"\n","    return add_documents_from_text(text, source_name=Path(p).name)\n","\n","\n","def answer_question(query: str) -> str:\n","    # Retrieve top contexts\n","    results = INDEX.search(query, k=TOP_K)\n","    contexts = [r[0] for r in results]\n","    # Generate answer\n","    answer = GEN.generate_answer(query, contexts)\n","    # return answer with sources\n","    srcs = [r[2].get('source', f'chunk_{i}') for i, r in enumerate(results)]\n","    meta_str = '\\n'.join([f\"Context {i+1} score={r[1]:.3f} source={r[2].get('source','-')}\" for i, r in enumerate(results)])\n","    return f\"Answer:\\n{answer}\\n\\nRetrieved contexts:\\n{meta_str}\"\n","\n","\n","def summarize_document(doc_index: int = 0) -> str:\n","    # Summarize a particular document chunk (doc_index in DOCUMENTS)\n","    _, text = DOCUMENTS[doc_index]\n","    return GEN.summarize(text)\n","\n","\n","def generate_quiz_from_doc(doc_index: int = 0, num_questions: int = 5) -> str:\n","    _, text = DOCUMENTS[doc_index]\n","    return GEN.generate_quiz(text, num_questions=num_questions)\n","\n","# ----------------- Gradio UI -----------------\n","\n","def launch_gradio(hf_token: str = None):\n","    init(hf_token=hf_token)\n","\n","    with gr.Blocks() as demo:\n","        gr.Markdown(\"# GenAI Study Assistant (Colab Demo)\")\n","        with gr.Row():\n","            with gr.Column():\n","                upload = gr.File(label=\"Upload document (txt / pdf)\")\n","                btn_upload = gr.Button(\"Upload & Index\")\n","                txt_manual = gr.Textbox(lines=8, label=\"Or paste text here\")\n","                btn_add_text = gr.Button(\"Add pasted text\")\n","                btn_init = gr.Button(\"(Re)initialize models\")\n","            with gr.Column():\n","                query = gr.Textbox(label=\"Ask a question\")\n","                btn_ask = gr.Button(\"Ask\")\n","                out_qa = gr.Textbox(label=\"Answer & Retrieved contexts\", lines=8)\n","\n","        with gr.Row():\n","            doc_index = gr.Number(value=0, label=\"Document chunk index (for summarization / quiz)\")\n","            btn_summary = gr.Button(\"Summarize chunk\")\n","            out_summary = gr.Textbox(label=\"Summary\", lines=6)\n","            btn_quiz = gr.Button(\"Generate quiz from chunk\")\n","            out_quiz = gr.Textbox(label=\"Quiz\", lines=8)\n","\n","        # Callbacks\n","        btn_init.click(lambda hf=hf_token: init(hf), inputs=[], outputs=[gr.Textbox(visible=False)])\n","        btn_upload.click(upload_and_index, inputs=[upload], outputs=[out_summary])\n","        btn_add_text.click(lambda t: add_documents_from_text(t, source_name='pasted_text'), inputs=[txt_manual], outputs=[out_summary])\n","        btn_ask.click(answer_question, inputs=[query], outputs=[out_qa])\n","        btn_summary.click(lambda idx: summarize_document(int(idx)), inputs=[doc_index], outputs=[out_summary])\n","        btn_quiz.click(lambda idx: generate_quiz_from_doc(int(idx), num_questions=5), inputs=[doc_index], outputs=[out_quiz])\n","\n","    demo.launch(share=False)\n","\n","# ----------------- If run as script -----------------\n","if __name__ == '__main__':\n","    print(\"This is a module. To run the Gradio demo, call launch_gradio(hf_token='<YOUR_HF_TOKEN>')\")\n","    # Example: launch_gradio(hf_token=os.getenv('HF_TOKEN'))\n","\n","launch_gradio(hf_token=HF_TOKEN)\n","\n","\n","\n"]}]}